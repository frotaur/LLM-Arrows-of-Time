{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Notebook used to compute\n",
    "logperplexity differences for snippets of text\n",
    "\"\"\"\n",
    "\n",
    "from modules import get_tokenizer\n",
    "from modules import MinGPT_Trainer, MinGPT\n",
    "import os,torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tokenizer_location = 'modules/tokenizers/en_tokenizer'\n",
    "\n",
    "f_model_location = 'saved_models/en_med'\n",
    "f_model_name = 'en_med'\n",
    "\n",
    "b_model_location = 'saved_models/en_med_b'\n",
    "b_model_name = 'en_med_b'\t\n",
    "\n",
    "toki = get_tokenizer(m_path=tokenizer_location)\n",
    "\n",
    "def load_model(model_location,model_name):\n",
    "    \"\"\"\n",
    "    Loads a model which has been exported with\n",
    "    export_model.\n",
    "\n",
    "    Args :\n",
    "        model_location : folder containing exported .py and .config files.\n",
    "        model_name : name of exported model, i.e. [name].py\n",
    "    \"\"\"\n",
    "    model = MinGPT(**torch.load(os.path.join(model_location,model_name+'.config')))\n",
    "    model.load_state_dict(torch.load(os.path.join(model_location,model_name+'.pt')))\n",
    "    return model\n",
    "\n",
    "def export_model(state_path, save_location, model_name):\n",
    "    MinGPT_Trainer.save_model_from_state(state_path, save_location, name=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 405.50M\n",
      "Without head : 354.04M\n",
      "number of parameters: 405.50M\n",
      "Without head : 354.04M\n"
     ]
    }
   ],
   "source": [
    "model_f = load_model(f_model_location,f_model_name)\n",
    "model_b = load_model(b_model_location,b_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved weights of en_med.pt at saved_models/en_med_b/en_med.pt  !\n"
     ]
    }
   ],
   "source": [
    "export_model('en_med_backwards.state','saved_models/en_med_b','en_med')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_bos(text,tokenizer,backward=False):\n",
    "    text = tokenizer.tokenize(text)\n",
    "\n",
    "    if(backward):\n",
    "        text=torch.flip(text,[1])\n",
    "    # return text\n",
    "    return torch.cat([torch.tensor([[0]]),text],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once upon a time, there was a prince. He was absol\n",
      "once upon a time, there was a prince. He was absol\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_loc = 'sample_text.txt'\n",
    "text_to_test = open(text_loc).read()\n",
    "\n",
    "tokenized = tokenize_with_bos(text_to_test,toki)\n",
    "inv_tokenized = tokenize_with_bos(text_to_test,toki,backward=True)\n",
    "# print(tokenized)\n",
    "# print(inv_tokenized)\n",
    "\n",
    "\n",
    "print(toki.detokenize(tokenized)[:50])\n",
    "print(toki.detokenize(inv_tokenized.flip([1]))[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of text :   best there was, no question.\n",
      "end of text :   wasere th, time a upononce\n",
      "Forward perp:  tensor([4.4576e+00, 1.5666e-02, 2.5615e-02, 1.7123e+00, 2.1794e+00, 3.3385e-04,\n",
      "        8.7502e-01, 7.2953e-05, 4.9712e-01, 5.8143e-01, 7.0945e+00, 3.2831e+00,\n",
      "        2.5014e+00, 9.4023e-01, 7.6368e+00, 3.2378e+00, 4.5349e-04, 3.2358e+00,\n",
      "        6.3885e+00, 3.8427e-03, 8.4452e-01, 3.6221e-04, 4.8725e-01, 1.7730e+00,\n",
      "        4.8164e+00, 2.3415e+00, 9.9012e-01])  :  tensor(2.0711)\n",
      "Back perp:  tensor([7.5085e+00, 4.9830e+00, 6.2113e-01, 7.5087e+00, 2.5591e+00, 1.1853e-03,\n",
      "        7.3156e-04, 1.2159e-02, 5.4379e+00, 2.2889e-01, 1.5461e-02, 6.2103e+00,\n",
      "        4.3245e-01, 3.9755e+00, 1.5103e-01, 1.0787e+01, 1.7348e+00, 2.4978e+00,\n",
      "        3.1441e+00, 2.6153e-02, 5.3300e-03, 1.4460e-02, 1.3916e+00, 3.7435e+00,\n",
      "        2.5579e-01, 3.6349e-03, 5.6981e+00])  :  tensor(2.5536)\n"
     ]
    }
   ],
   "source": [
    "def get_perplexities(text,model,tokenizer,backward=False):\n",
    "    text = tokenize_with_bos(text,tokenizer,backward) # (1,seq_len)\n",
    "    model.eval()\n",
    "\n",
    "    if(text.shape[1]>256):\n",
    "        print('text too long, truncating')\n",
    "        text = text[:,:256]\n",
    "    print('end of text : ',toki.detokenize(text[:,-10:]))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(text) # (1,seq_len,vocab_size)\n",
    "        B,T,V = out.size()\n",
    "        out = out.reshape(B*T,V)\n",
    "        text = text.reshape(B*T)\n",
    "        loss = F.cross_entropy(out[:-1,:], text[1:] ,reduction='none')\n",
    "        return loss\n",
    "\n",
    "f_perp = get_perplexities(text_to_test,model_f,toki,backward=False)\n",
    "b_perp = get_perplexities(text_to_test,model_b,toki,backward=True)\n",
    "\n",
    "print('Forward perp: ', f_perp, ' : ', f_perp.mean())\n",
    "print('Back perp: ', b_perp, ' : ', b_perp.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
